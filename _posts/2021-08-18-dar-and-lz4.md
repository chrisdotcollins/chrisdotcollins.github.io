---
title: Data Compression
#subtitle: 
date: 2021-08-18
categories: ["bash", "compress", "tar", "dar", "lz4"]
layout: single
classes: wide
---
# Data Storage #

Breaks down into two tasks:
* Compression of large data files / sets to reduce on disk storage or improve transfer speed
* Consolidate many file directories into a more managable single file for archiving/moving.


# Compression #
Looking at reducing the storage space of some large datasets which was effectively cold / pre-delete. Usually I'd go for gzip as it is available pretty much everywhere and seems relatively quick and efficient. However, looking at this particular dataset gzip was taking a fair amount of time to compress a single file, so would have taken an age to compress a large (hundreds or even thousands of files) dataset.

Using a typical file from the dataset I was working at the time, I started to investigate the different options, namely the familiar gzip and bz2, the increasingly popular xz and then a new one to me, lz4. The original file was about 12GB in size, and the benchmark to work against (gzip) was 12 minute Compression time to create an 8GB output.

```
time gzip datafile_1.sim
real	12m2.703s

time bzip2 -1 datafile_1.sim
real	25m21.603s

time xz -1 -z datafile_1.sim
real	30m14.539s

time xz -6 -z datafile_1.sim
real	83m32.735s

time datafile_1.sim datafile_1.sim.lz4
Compressed 12701376948 bytes into 9553258174 bytes ==> 75.21%                  
real	0m34.384s

time lz4 -9 datafile_1.sim datafile_1.sim.lz4-9
Compressed 12701376948 bytes into 9116591892 bytes ==> 71.78%                  
real	7m7.716s
```
Some interesting results here, with the default lz4 compression settings outputing a compressed file in just 34 seconds. How did this compare though in terms of compression rate / file size? Pretty well to be fair:

The resulting filesizes
```
12701376948 Jul 21 12:34 datafile_1.sim
 9553258174 Jul 21 12:34 datafile_1.sim.lz4       (75%)
 9116591892 Jul 21 12:34 datafile_1.sim.lz4-9     (72%)
 8082204196 Jul 21 12:34 datafile_1.sim.bz2       (64%)
 8071256709 Jul 21 11:53 datafile_1.sim.gzip      (64%)
 7306353832 Jul 21 14:53 datafile_1.sim.xz.1      (58%)
 7040715848 Jul 21 16:08 datafile_1.sim.xz.6      (55%)
```
Clearly if you care about compression, then go hardcore settings with xz and be prepared for some long coffee breaks waiting for your files to compress (though is it worth the extra time over standard xz?). However, if you want decent compression while getting some of your precious time back, lz4 does a pretty decent job in VERY little time. 
# File count
As part of the same exercise, the next big thing to look at were some user directories where the data management had gone out of the window along with any concideration around practical file counts in directories.

The objective here was to create an archive of the files that was still usable, but meant the filesize could be reduced and the archive migrated in a realistic timespan. The go to in this case is good old tar, but tar has its limitations.It is designed for tape (the t in tar(chive)) and is great for streaming media, but that doesn't facilitate random access very easily. Say you have an archive of 10,000 files, and you want a random file, all 10k of it, from somewhere in the archive, then generally you have to stream the whole archive out to get your file (even if you don't _see_ those files. Sure, there are ways around that if you really want, but it sort of gets complicated.

Instead, I took a look at a command dar, which is the disk archive to tar's tape archive. The end point is similiar, you get a 'darball' (is that the word) but this is indexed, so if you want your file out of the middle, you get it out of the middle in next to no time.


Using a tarball of a sample set I created, I tried to extract a single tif file. With tar, this took just over 6 minutes.
```
# time tar -xf Data.tar path/to/my/file.tif

real    6m15.813s
```
Doing the same thing with dar, though with a much more complicated set of options. This is one of the areas dar negatively differs from tar, it isn't nearly as intuitive. Below I've run with the verbose flag to give an idea of what is going on:
```
# time dar -R restore -O -w -x Data-dar -v -g path/to/my/file.tif
Arguments read from /etc/darrc :

Opening archive Data-dar ...
Opening the archive using the multi-slice abstraction layer...
Reading the archive trailer...
Opening construction layer...
Considering cyphering layer...
No cyphering layer opened
Opening escape sequence abstraction layer...
Opening the compression layer...
All layers have been created successfully
Loading catalogue into memory...
Locating archive contents...
Reading archive contents...
Restoring file's data: /home/cc/Data/
Restoring file's data: /home/cc/Data/path/
Restoring file's data: /home/cc/Data/path/to/my
Restoring file's data: /home/cc/Data/path/to/my/file.tif

 --------------------------------------------
 4 inode(s) restored
    including 0 hard link(s)
 0 inode(s) not restored (not saved in archive)
 0 inode(s) not restored (overwriting policy decision)
 45 inode(s) ignored (excluded by filters)
 0 inode(s) failed to restore (filesystem error)
 0 inode(s) deleted
 --------------------------------------------
 Total number of inode(s) considered: 49
 --------------------------------------------
 EA restored for 0 inode(s)
 FSA restored for 0 inode(s)
 --------------------------------------------
Final memory cleanup...

real	0m2.401s
```
In this example, the file was extracted from the darball in about 2 seconds. That then becomes a useable option.



# Related tasks
Other thing to consider:
* MPI File Utils
* Faster deletion